{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Regression\n",
    "## House price prediction case study\n",
    "\n",
    "#### Problem Statement:\n",
    "A US-based housing company named Surprise Housing has decided to enter the Australian market. The company uses data analytics to purchase houses at a price below their actual values and flip them on at a higher price. For the same purpose, the company has collected a data set from the sale of houses in Australia. The data is provided in the CSV file below.\n",
    "\n",
    "The company is looking at prospective properties to buy to enter the market. You are required to build a regression model using regularisation in order to predict the actual value of the prospective properties and decide whether to invest in them or not.\n",
    "\n",
    "The company wants to know:\n",
    "\n",
    "    1. Which variables are significant in predicting the price of a house, and\n",
    "    2. How well those variables describe the price of a house.\n",
    "\n",
    "Also, determine the optimal value of lambda for ridge and lasso regression.\n",
    "\n",
    " \n",
    "\n",
    "#### Business Goal:\n",
    "You are required to model the price of houses with the available independent variables. This model will then be used by the management to understand how exactly the prices vary with the variables. They can accordingly manipulate the strategy of the firm and concentrate on areas that will yield high returns. Further, the model will be a good way for management to understand the pricing dynamics of a new market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brief outline of different steps involved in modelling:\n",
    "1. Partitioning the data into train/validation/test chunks \n",
    "2. Load the data and understand variables \n",
    "3. Data inspection\n",
    "4. Exploratory Data Analysis\n",
    "5. Pre-processing data - missing value imputation, scaling, dropping variables, etc.\n",
    "6. Modelling using Regression\n",
    "7. Hyper parameter tuning and regularization - ridge/lasso\n",
    "8. Model evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and inspecting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we already have the data in the form of train.csv and test.csv, we can go ahead and skip the step where we split the data. We will use the train.csv to do understand, preprocess and perform some EDA on it in the next few steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model, metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = pd.read_csv('train.csv')\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's also take a look at the shape of the data\n",
    "housing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some info about data types of the variables\n",
    "housing.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary of the data\n",
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have about 81 columns and 1460 rows in total. 38 of the columns are of numeric type(float and int64) and 43 of them are of object type(strings or characters, dates). We can also see that some of the columns have null values as well, we will perform some pre-processing operations to treat them in the subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's take a look at some of the numeric variables \n",
    "housing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us some summary of distribution of numeric variables. Refering to the data dictionary, we can identify some important variables and how the mean, median and inter-quartile ranges are distributed as well as presence of some outliers. Some of the variables that could be important to our modelling - \n",
    "1. LotArea - which ranges from 1300 sq.feet to 215245 sq.feet, having a median of 9478 sq.feet\n",
    "2. OverallQual and OverallCond - ratings of material/finish and condition of the house which ranges from 1-10, 1 being the worst and 10 being the best.\n",
    "3. YearBuilt - Year in which the house is built, ranging from 1872 to 2010, with average year of building of the house being 1971.\n",
    "\n",
    "These are only a few of the numerical variables present, we shall explore more and visualise them in the later steps to see what all could be important for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the data dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some of the variables that have sub-categories as mentioned in the data dictonary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSSubClass has various categories under it which are all represented as numeric variables. Let's see what it looks like\n",
    "\n",
    "ms_subclass_dic = { \n",
    "        20:\t\"1-STORY 1946 & NEWER ALL STYLES\",\n",
    "        30:\t\"1-STORY 1945 & OLDER\",\n",
    "        40:\t\"1-STORY W/FINISHED ATTIC ALL AGES\",\n",
    "        45:\t\"1-1/2 STORY - UNFINISHED ALL AGES\",\n",
    "        50:\t\"1-1/2 STORY FINISHED ALL AGES\",\n",
    "        60:\t\"2-STORY 1946 & NEWER\",\n",
    "        70:\t\"2-STORY 1945 & OLDER\",\n",
    "        75:\t\"2-1/2 STORY ALL AGES\",\n",
    "        80:\t\"SPLIT OR MULTI-LEVEL\",\n",
    "        85:\t\"SPLIT FOYER\",\n",
    "        90:\t\"DUPLEX - ALL STYLES AND AGES\",\n",
    "       120:\t\"1-STORY PUD (Planned Unit Development) - 1946 & NEWER\",\n",
    "       150:\t\"1-1/2 STORY PUD - ALL AGES\",\n",
    "       160:\t\"2-STORY PUD - 1946 & NEWER\",\n",
    "       180:\t\"PUD - MULTILEVEL - INCL SPLIT LEV/FOYER\",\n",
    "       190:\t\"2 FAMILY CONVERSION - ALL STYLES AND AGES\"\n",
    "    }\n",
    "\n",
    "ms_subclass = housing['MSSubClass'].astype('category').value_counts()\n",
    "#let's map the category description to get a better idea\n",
    "ms_subclass = ms_subclass.to_frame().reset_index()\n",
    "ms_subclass['index'] = ms_subclass['index'].map(ms_subclass_dic)\n",
    "\n",
    "#let's take a look at the value counts of different categories present\n",
    "print(ms_subclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's repeat the same thing for the variable MSZoning\n",
    "ms_zoning_dic = {\n",
    "       \"A\":\t\"Agriculture\",\n",
    "       \"C (all)\":\t\"Commercial\",\n",
    "       \"FV\":\t\"Floating Village Residential\",\n",
    "       \"I\":\t\"Industrial\",\n",
    "       \"RH\":\t\"Residential High Density\",\n",
    "       \"RL\":\t\"Residential Low Density\",\n",
    "       \"RP\":\t\"Residential Low Density Park\", \n",
    "       \"RM\":\t\"Residential Medium Density\"\n",
    "}\n",
    "\n",
    "ms_zoningclass = housing['MSZoning'].astype('category').value_counts()\n",
    "#let's map the category description to get a better idea\n",
    "ms_zoningclass = ms_zoningclass.to_frame().reset_index()\n",
    "ms_zoningclass['index'] = ms_zoningclass['index'].map(ms_zoning_dic)\n",
    "\n",
    "#let's take a look at the value counts of different categories present\n",
    "print(ms_zoningclass)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis\n",
    "In this step, let's visualise and make sense of the different numeric and categorical variables in our data with the help of differnt plots and statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all numeric (float and int) variables in the dataset\n",
    "housing_num = housing.select_dtypes(include=['float64', 'int64'])\n",
    "housing_num.head()\n",
    "# [['LotFrontage', 'LotArea', 'YearBuilt', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'GarageArea', 'YrSold']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the variables like ```MSSubClass``` are represented numerically, but they have discrete categories that they're mapped to. We shall drop some of the variables and convert them to categorical variables in the later steps. For now let's drop them and visualise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num = housing_num.drop(['MSSubClass','Id', 'OverallQual', 'OverallCond', 'MoSold'], axis=1)\n",
    "housing_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's first understand how different feature variables are related to the target variable (features vs saleprice) \n",
    "# by plotting and visualising them.\n",
    "sns.scatterplot(housing_num['LotFrontage'], housing_num['SalePrice'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Looking at the above plot, there is a cluster of values between 0-150 feet of LotFootage and 0-400000 dollars. Even though the relationship might not look strictly linear, we could draw a straight line through the points and explain the data.\n",
    "2. We can also spot the outliers in the data - there are two data points at the extreme end of LotFrontage and two points which exceed the sale price of ```700,000$```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(housing_num['LotArea'], housing_num['SalePrice'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The plot of ```LotArea``` vs ```SalePrice``` is different from what we observed in the first case. The distribution is more compact and for a given value of area, the price ranges from under 100000 to over 400000 dollars. We could say that the relationship is not strictly linear and there is perhaps a more complicated mapping of feature variable to the target variable.\n",
    "2. We can also spot the outliers from this plot - the ones having lot area exceeding 150000 sq.ft and the ones exceeding sale price of 700000 dollars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, let's select some of the variables and visualise pairplots between them to get a better idea of relationship between 2 or more variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num_filtered = housing_num[['LotFrontage', 'LotArea', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'GarageArea', 'SalePrice']]\n",
    "sns.pairplot(housing_num_filtered)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is a set of pair-plots that show the relationship between each of the variables with one another. Let's particularly focus on the last row of plots which show us the relationship between ```SalePrice``` and other features.\n",
    "1. When we look at total basement sq.feet vs price plot, we can observe that there are various data points which have the value 0 on the x-axis, meaning these are houses that do not have a basement.\n",
    "2. The plot between second floor sq.feet vs price also has data points corresponding to 0 on the x-axis, meaning there are a number of houses that do not comprise of a second floor. This is observed with garage area vs price as well.\n",
    "3. The plots of ```1stFlrSF```, ```2ndFlrSF``` and ```GrLivArea``` have somewhat of a linear increasing trend. As the area increases, we also see prices go up. Similar trends are observed with ```GarageArea``` vs ```SalePrice``` as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the relationship between sale prices and the year in which the house was built/sold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert YearBuilt and SalePrice to categorical variables\n",
    "\n",
    "\n",
    "#plot price vs year built and year sold\n",
    "sns.histplot(data=housing_num, x=\"YearBuilt\", y=\"SalePrice\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot price vs year built and year sold\n",
    "sns.histplot(data=housing_num, x=\"YrSold\", y=\"SalePrice\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the categorical variables and some relevant plots for analyse them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all categorical (object) variables in the dataset\n",
    "housing_cpy = housing.copy()\n",
    "housing_cpy['Id'] = housing_cpy['Id'].astype(str)\n",
    "\n",
    "housing_cat = housing_cpy.select_dtypes(include=['object'])\n",
    "housing_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, some of the variables are represented numerically even though they occupy certain discrete values to represent the data. Let's add them to the dataframe ```housing_cat``` and convert them to object types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num = housing.select_dtypes(include=['float64', 'int64'])\n",
    "housing_num_filtered = housing_num[['MSSubClass', 'Id', 'OverallQual', 'OverallCond', 'MoSold', 'SalePrice']]\n",
    "\n",
    "housing_cat['Id'] = housing_cat['Id'].astype('int64')\n",
    "housing_cat_merge = pd.merge(housing_num_filtered, housing_cat, how='inner', on='Id')\n",
    "housing_cat_merge.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert numeric types to object types\n",
    "housing_cat_merge['MSSubClass'] = housing_cat_merge['MSSubClass'].astype(str)\n",
    "\n",
    "#let's plot type of dwelling involved in the sale vs prßice the house is sold using a box plot\n",
    "plt.figure(figsize=(60, 20))\n",
    "plt.subplot(2,4,1)\n",
    "sns.boxplot(x = 'MSSubClass', y = 'SalePrice', data = housing_cat_merge)\n",
    "\n",
    "#plot zoning type vs price \n",
    "plt.subplot(2,4,2)\n",
    "sns.boxplot(x = 'MSZoning', y = 'SalePrice', data = housing_cat_merge)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. category ```60``` (2-STORY 1946 & NEWER) seems to have higher median and quartile values of house price than all other categories, followed by ```120```(1-STORY PUD (Planned Unit Development) - 1946 & NEWER)   and ```75```(2-1/2 STORY ALL AGES). \n",
    "2.  \n",
    "3. We can also look at how many outliers are there in our data, which are the black points that exceed Q4.\n",
    "4. The right plot shows us zoning type vs sale price. ```FV```(Floating Village Residential) has the maximum median sale price followed by ```RL```(Residential Low Density). Commercial property seems to have the least median sale price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(60, 20))\n",
    "plt.subplot(2,4,1)\n",
    "sns.boxplot(x = 'OverallQual', y = 'SalePrice', data = housing_cat_merge)\n",
    "\n",
    "plt.subplot(2,4,2)\n",
    "sns.boxplot(x = 'OverallCond', y = 'SalePrice', data = housing_cat_merge)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plots show us the trend of overall quality and condition vs sale price. \n",
    "1. The left plot is like how we would expect it to be - i.e. higher the rating of house quality, higher the price of the house and we can see a linear increase in the values.\n",
    "2. The plot on the right however, has steadily increasing median values till ```5``` after which the value drops and remains almost constant for categories ```6```,```7``` and ```8```. We see an increase in sale prices for category ```9```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(60, 20))\n",
    "plt.subplot(2,4,1)\n",
    "sns.boxplot(x = 'BldgType', y = 'SalePrice', data = housing_cat_merge)\n",
    "\n",
    "# plt.subplot(2,4,2)\n",
    "# sns.boxplot(x = 'Neighborhood', y = 'SalePrice', data = housing_cat_merge)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get numeric variables to see how they're correlated.\n",
    "housing_num_fil = housing_num.drop(['MSSubClass','Id', 'OverallQual', 'OverallCond', 'MoSold'], axis=1)\n",
    "housing_num_fil.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now take a look at how the variables are correlated by plotting a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (40, 20))\n",
    "sns.heatmap(housing_num_fil.corr(), annot = True, cmap=\"YlGnBu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data pre-processing and preparation\n",
    "\n",
    "Earlier when we had explored the dataset, we observed some rows and columns having null values, categorical data being cast as numerical values and so on. Here, we will treat these issues present in the data before making it ready for modelling. Some of the steps performed include:\n",
    "1. missing/null value imputation\n",
    "2. dropping rows/columns if necessary\n",
    "3. dummy variables for categorical variables\n",
    "4. scaling of numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets check missing values in our data\n",
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove columns which have more than ```85%``` null values in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_na = housing.dropna(thresh=len(housing)*0.85 , axis=1)\n",
    "housing_na.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 74 different columns now after dropping columns with null values greater than treshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's look at columns which still have some missing values and perform imputation depending on the nature of data.\n",
    "\n",
    "#select columns which still have some missing values\n",
    "df_null = housing_na.loc[:, housing_na.isnull().any()]      #this gives us the\n",
    "df_null.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's fill the null values in all of the above columns\n",
    "values = {\"MasVnrType\": df_null.mode(dropna=True)['MasVnrType'][0], \n",
    "            \"MasVnrArea\":df_null.median(skipna=True)['MasVnrArea'],\n",
    "            \"BsmtQual\": df_null.mode(dropna=True)['BsmtQual'][0], \n",
    "            \"BsmtCond\":df_null.mode(dropna=True)['BsmtCond'][0],\n",
    "            \"BsmtExposure\": df_null.mode(dropna=True)['BsmtExposure'][0], \n",
    "            \"BsmtFinType1\":df_null.mode(dropna=True)['BsmtFinType1'][0],\n",
    "            \"BsmtFinType2\": df_null.mode(dropna=True)['BsmtFinType2'][0], \n",
    "            \"Electrical\":df_null.mode(dropna=True)['Electrical'][0],\n",
    "            \"GarageType\": df_null.mode(dropna=True)['GarageType'][0], \n",
    "            \"GarageYrBlt\":df_null.median(skipna=True)['GarageYrBlt'],\n",
    "            \"GarageFinish\": df_null.mode(dropna=True)['GarageFinish'][0], \n",
    "            \"GarageQual\":df_null.mode(dropna=True)['GarageQual'][0],\n",
    "            \"GarageCond\":df_null.mode(dropna=True)['GarageCond'][0]\n",
    "        }\n",
    "housing_na = housing_na.fillna(value=values)\n",
    "housing_na.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have non-null values in all the columns. Let's proceed to perform the other pre-processing steps as mentioned earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy variable creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets split the data now into X(feature variables) and Y(target variable) before creating dummy variables\n",
    "Y = housing_na['SalePrice']\n",
    "X = housing_na.drop(['Id', 'SalePrice'],axis=1)\n",
    "print(X.shape, Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cpy = X.copy()\n",
    "\n",
    "#lets convert the categorical variables that have been represented as numeric values\n",
    "X_cpy['MSSubClass'] = X_cpy['MSSubClass'].astype(str)\n",
    "X_cpy['OverallQual'] = X_cpy['OverallQual'].astype(str)\n",
    "X_cpy['OverallCond'] = X_cpy['OverallCond'].astype(str)\n",
    "X_cpy['MoSold'] = X_cpy['MoSold'].astype(str)\n",
    "\n",
    "X_cat = X_cpy.select_dtypes(include=['object'])\n",
    "X_cat.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the category name of MSSubClass based on data dict\n",
    "\n",
    "ms_subclass_dic = { \n",
    "        \"20\":\t\"1-STORY 1946 & NEWER ALL STYLES\",\n",
    "        \"30\":\t\"1-STORY 1945 & OLDER\",\n",
    "        \"40\":\t\"1-STORY W/FINISHED ATTIC ALL AGES\",\n",
    "        \"45\":\t\"1-1/2 STORY-UNFINISHED ALL AGES\",\n",
    "        \"50\":\t\"1-1/2 STORY FINISHED ALL AGES\",\n",
    "        \"60\":\t\"2-STORY 1946 & NEWER\",\n",
    "        \"70\":\t\"2-STORY 1945 & OLDER\",\n",
    "        \"75\":\t\"2-1/2 STORY ALL AGES\",\n",
    "        \"80\":\t\"SPLIT OR MULTI-LEVEL\",\n",
    "        \"85\":\t\"SPLIT FOYER\",\n",
    "        \"90\":\t\"DUPLEX-ALL STYLES AND AGES\",\n",
    "       \"120\":\t\"1-STORY PUD-1946 & NEWER\",\n",
    "       \"150\":\t\"1-1/2 STORY PUD - ALL AGES\",\n",
    "       \"160\":\t\"2-STORY PUD-1946 & NEWER\",\n",
    "       \"180\":\t\"PUD-MULTILEVEL-INCL SPLIT LEV/FOYER\",\n",
    "       \"190\":\t\"2 FAMILY CONVERSION-ALL STYLES AND AGES\"\n",
    "    }\n",
    "\n",
    "X_cat['MSSubClass'] = X_cat['MSSubClass'].map(ms_subclass_dic)\n",
    "X_cat.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into dummies - one hot encoding\n",
    "X_cat_dummies = pd.get_dummies(X_cat, drop_first=True)\n",
    "X_cat_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop categorical variables \n",
    "X = X.drop(list(X_cat.columns), axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat dummy variables with X\n",
    "X = pd.concat([X, X_cat_dummies], axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created dummy variables, let's move to the step of scaling these values using a StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "cols = X.columns.to_list()\n",
    "print(cols)\n",
    "X[cols] = scaler.fit_transform(X[cols])\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have scaled all of our values, let's go back to seeing correlation between variables as some of the variables can be dropped based on their correlation with other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (40, 20))\n",
    "sns.heatmap(housing_num_fil.corr(), annot = True, cmap=\"YlGnBu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above heatmap, we can observe two things.\n",
    "1. ```SalePrice``` is correlated with a number of other features.\n",
    "2. There is some correlation between the independent variables as well. We shall identify them and drop some of it before modelling as it will help us reduce the number of features in our model, thereby preventing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num_fil = housing_num_fil.drop(['SalePrice'],axis=1)\n",
    "corr_mat = housing_num_fil.corr().abs()\n",
    "corr_mat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the upper traingle as the matrix is mirror image about the diagonal\n",
    "upper_tri = corr_mat.where(np.triu(np.ones(corr_mat.shape),k=1).astype(np.bool))\n",
    "print(upper_tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.70)]\n",
    "print(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(to_drop,axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modelling\n",
    "\n",
    "Let's dive into building a model using linear regression, first with a simple LinearRegression combined with RFE and evaluate it using metrics like RMSE and r2 scores. We will follow that up with using ridge and lasso regression and hyper parameter tuning to see how our model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#sanity check\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "#we'll use the sklearn LinearRegression estimator for RFE \n",
    "lm = LinearRegression()\n",
    "rfe = RFE(lm, n_features_to_select=60)             # running RFE\n",
    "rfe = rfe.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of features from rfe\n",
    "list(zip(X.columns,rfe.support_,rfe.ranking_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = X.columns[rfe.support_]\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns[~rfe.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X_test dataframe with RFE selected variables\n",
    "X_train_rfe = X[col]\n",
    "X_train_rfe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the new dataset \n",
    "\n",
    "# Add a constant\n",
    "X_train_lm = sm.add_constant(X_train_rfe)\n",
    "\n",
    "# Create a first fitted model\n",
    "lr = sm.OLS(Y, X_train_lm).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's check the intercepts and coeffecients \n",
    "lr.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a summary of the linear regression model obtained\n",
    "print(lr.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the const column before calculating VIF\n",
    "X_train_lm.drop(['const'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the VIFs for the new model\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "X_new = X_train_lm\n",
    "vif['Features'] = X_new.columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_new.values, i) for i in range(X_new.shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of our features have a VIF of ```inf```, which means that there is high levels of correlation. Let's drop the columns that have infinite VIF and retrain our model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = vif['Features'].where(vif['VIF'] == float('inf'))\n",
    "cols_to_drop.dropna(inplace=True)\n",
    "cols_to_drop = cols_to_drop.to_list()\n",
    "cols_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = X_new.drop(columns=cols_to_drop)\n",
    "X_train_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's fit the model again with new set of features\n",
    "\n",
    "# Add a constant\n",
    "X_train_lm = sm.add_constant(X_train_new)\n",
    "\n",
    "# Create a first fitted model\n",
    "lr = sm.OLS(Y, X_train_lm).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see the summary of the new model\n",
    "print(lr.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the const column before calculating VIF\n",
    "X_train_lm.drop(['const'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif = pd.DataFrame()\n",
    "X_new = X_train_lm\n",
    "vif['Features'] = X_new.columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_new.values, i) for i in range(X_new.shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still features which have a high VIF. Let's remove them and repeat the same process again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['RoofMatl_CompShg', 'MasVnrType_None', 'MasVnrType_BrkFace', 'Exterior2nd_CmentBd', 'Exterior1st_CemntBd']\n",
    "X_train_new = X_new.drop(columns=cols_to_drop)\n",
    "X_train_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's fit the model again with new set of features\n",
    "\n",
    "# Add a constant\n",
    "X_train_lm = sm.add_constant(X_train_new)\n",
    "\n",
    "# Create a first fitted model\n",
    "lr = sm.OLS(Y, X_train_lm).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see the summary of the new model\n",
    "print(lr.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the const column before calculating VIF\n",
    "X_train_lm.drop(['const'], axis=1, inplace=True)\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "X_new = X_train_lm\n",
    "vif['Features'] = X_new.columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_new.values, i) for i in range(X_new.shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets remove features which have VIF>5\n",
    "cols_to_drop = ['GarageType_Attchd', 'GarageType_Detchd', 'BsmtQual_TA']\n",
    "X_train_new = X_new.drop(columns=cols_to_drop)\n",
    "X_train_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's fit the model again with new set of features\n",
    "\n",
    "# Add a constant\n",
    "X_train_lm = sm.add_constant(X_train_new)\n",
    "\n",
    "# Create a first fitted model\n",
    "lr = sm.OLS(Y, X_train_lm).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the const column before calculating VIF\n",
    "X_train_lm.drop(['const'], axis=1, inplace=True)\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "X_new = X_train_lm\n",
    "vif['Features'] = X_new.columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_new.values, i) for i in range(X_new.shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the summary of our model. In specific, we'll take a look at the p-values of coeffecients to determine if they're significant. Coeffecients with more than 0.05 p-value can be rejected and we can retain the remaining variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lr.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's evaluate the base model\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "metrics_lr = []\n",
    "y_pred_train = lr.predict(X_train_rfe)\n",
    "\n",
    "r2_train_lr = r2_score(Y, y_pred_train)\n",
    "print(\"R2 Score:\", r2_train_lr)\n",
    "metrics_lr.append(r2_train_lr)\n",
    "\n",
    "rss1_lr = np.sum(np.square(Y - y_pred_train))\n",
    "print(\"RSS value:\", rss1_lr)\n",
    "metrics_lr.append(rss1_lr)\n",
    "\n",
    "mse_train_lr = mean_squared_error(Y, y_pred_train)\n",
    "print(\"Mean squared error:\", mse_train_lr)\n",
    "metrics_lr.append(mse_train_lr**0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ridge regression\n",
    "\n",
    "# params = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, \n",
    "#  0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n",
    "#  4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000 ]}\n",
    "\n",
    "# ridge = Ridge()\n",
    "\n",
    "# # cross validation\n",
    "# folds = 5\n",
    "# model_cv = GridSearchCV(estimator = ridge, \n",
    "#                         param_grid = params, \n",
    "#                         scoring= 'neg_mean_absolute_error',  \n",
    "#                         cv = folds, \n",
    "#                         return_train_score=True,\n",
    "#                         verbose = 1)            \n",
    "# model_cv.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the best hyperparameter alpha\n",
    "# print(model_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting Ridge model for alpha = 500 and printing coefficients which have been penalised\n",
    "# alpha = 10\n",
    "# ridge = Ridge(alpha=alpha)\n",
    "\n",
    "# ridge.fit(X, Y)\n",
    "# print(ridge.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_train = ridge.predict(X)\n",
    "\n",
    "# metrics_ridge = []\n",
    "\n",
    "# r2_train_lr = r2_score(Y, y_pred_train)\n",
    "# print(\"R2 Score:\", r2_train_lr)\n",
    "# metrics_ridge.append(r2_train_lr)\n",
    "\n",
    "# rss1_lr = np.sum(np.square(Y - y_pred_train))\n",
    "# print(\"RSS value:\", rss1_lr)\n",
    "# metrics_ridge.append(rss1_lr)\n",
    "\n",
    "# mse_train_lr = mean_squared_error(Y, y_pred_train)\n",
    "# print(\"Mean squared error:\", mse_train_lr)\n",
    "# metrics_ridge.append(mse_train_lr**0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lasso regression\n",
    "\n",
    "# lasso = Lasso()\n",
    "\n",
    "# # cross validation\n",
    "# model_cv = GridSearchCV(estimator = lasso, \n",
    "#                         param_grid = params, \n",
    "#                         scoring= 'neg_mean_absolute_error', \n",
    "#                         cv = folds, \n",
    "#                         return_train_score=True,\n",
    "#                         verbose = 1)            \n",
    "\n",
    "# model_cv.fit(X, Y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the best hyperparameter alpha\n",
    "# print(model_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha =500\n",
    "# lasso = Lasso(alpha=alpha)        \n",
    "# lasso.fit(X, Y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_lasso = []\n",
    "\n",
    "# y_pred_train = lasso.predict(X)\n",
    "\n",
    "# r2_train_lr = r2_score(Y, y_pred_train)\n",
    "# print(\"R2 Score:\", r2_train_lr)\n",
    "# metrics_lasso.append(r2_train_lr)\n",
    "\n",
    "# rss1_lr = np.sum(np.square(Y - y_pred_train))\n",
    "# print(\"RSS value:\", rss1_lr)\n",
    "# metrics_lasso.append(rss1_lr)\n",
    "\n",
    "# mse_train_lr = mean_squared_error(Y, y_pred_train)\n",
    "# print(\"Mean squared error:\", mse_train_lr)\n",
    "# metrics_lasso.append(mse_train_lr**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise the different metrics in the form of a tabular column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating a table which contain all the metrics\n",
    "\n",
    "# lr_table = {'Metric': ['R2 Score (Train)','RSS (Train)',\n",
    "#                        'MSE (Train)'], \n",
    "#         'Linear Regression': metrics_lr\n",
    "#         }\n",
    "\n",
    "# lr_metric = pd.DataFrame(lr_table ,columns = ['Metric', 'Linear Regression'] )\n",
    "\n",
    "# rg_metric = pd.Series(metrics_ridge, name = 'Ridge Regression')\n",
    "# ls_metric = pd.Series(metrics_lasso, name = 'Lasso Regression')\n",
    "\n",
    "# final_metric = pd.concat([lr_metric, rg_metric, ls_metric], axis = 1)\n",
    "\n",
    "# final_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting the values of lambda for ridge and lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eecs498",
   "language": "python",
   "name": "eecs498"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
